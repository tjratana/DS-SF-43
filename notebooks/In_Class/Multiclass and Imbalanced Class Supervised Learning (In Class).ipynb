{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Sklearn Tools and Multi-Class and Imbalanced Class Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Goals: </b>\n",
    "\n",
    "- Learn how to use an array of advanced tools in sklearn such as Pipelines, GridSearch, and more\n",
    "- Work on a supervised classification dataset with more than two classes, specifically the famous MNIST digits dataset.\n",
    "- Work on a supervised classification dataset with imbalanced classes, specifically the credit card fraud dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Sklearn tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.metrics import recall_score, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.preprocessing import Imputer, OneHotEncoder, LabelEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in titanic data\n",
    "\n",
    "path = \"../../data/titanic.csv\"\n",
    "\n",
    "titanic = pd.read_csv(path)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding aka dummy variables with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One hot encoding:*** Transforming categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = titanic.drop(\"Survived\", axis = 1)\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a train test split with the titanic data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use LabelEncoder to turn the object values into numbers. Instead turning each unique value into a column a la dummy variables, this tool returns a single column and replaces the objects/strings with a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize LabelEncoder object\n",
    "\n",
    "\n",
    "\n",
    "#Use le on the sex column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns male and female into a 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using this is that we can use the LabelEncoder object (le) to transform other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass in the Sex column on the testing dataset into the le object\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try this on the Embarked column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Intialize LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "#Pass Embarked column into le object\n",
    "\n",
    "\n",
    "#Look at first twenty rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call .classes_ to see the original object values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the embarked class of the testing dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at original X_test.Embarked\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the OneHotEncoder object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Intialize object\n",
    "\n",
    "\n",
    "#Fit and transform using the emb_encoded variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Look at emb_onehot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform emb_encoded_test using onehot object\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the LabelBinarizer to do this as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Intialize LabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "#Fit and transform on the Embarked column of the training dataset\n",
    "\n",
    "#Fill nans with unknown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the class or column values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the testing data using lb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be asking yourself \"Why use this instead of pd.get_dummies?\"\n",
    "\n",
    "That's because testing data or any other new you want to use may not have the same values in their categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataset from X_test where there is no C value in the Embarked column\n",
    "X_test2 = X_test[X_test.Embarked != \"C\"]\n",
    "\n",
    "\n",
    "#Transform the Embarked column from  X_test2 using the LabelBinarizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a 0 for every value under the C column. Using pd.get_dummies we would have three columns instead of four. This is important because when you fit you model using the training data and then make predictions using the testing data, the model won't work if your testing and training data don't have the same number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion:\n",
    "\n",
    "![e](https://chrisalbon.com/images/machine_learning_flashcards/One-Hot_Encoding_print.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm that tests every combination of model parameters to find the best one.\n",
    "\n",
    "Let's use GridSearch to find the best K value for a KNN model and Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in spotify data and assign X/y\n",
    "spotify = pd.read_csv(\"../../data/spotify_data.csv\", index_col=[0])\n",
    "\n",
    "X = spotify.drop(\"target\", axis = 1)\n",
    "y = spotify.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize parameter grid\n",
    "\n",
    "#Range of neighbors to test\n",
    "\n",
    "\n",
    "\n",
    "#Dictionary of parameter values \n",
    "param_grid_knn = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "grid_knn = \n",
    "\n",
    "\n",
    "#Fit grid on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whats the best cross validated accuracy score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple technique gives us the best K value.\n",
    "\n",
    "We can use the best model from grid_knn to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input \n",
    "x = [[0.2, .15, 0.68, 0.05, 0.328]]\n",
    "\n",
    "#Make prediction \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Range of neighbors to test\n",
    "depths_range = range(2, 20)\n",
    "\n",
    "#Dictionary of parameter values \n",
    "param_grid_dt = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "grid_dt = \n",
    "\n",
    "\n",
    "#Fit grid on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score for DT model\n",
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter for DT model\n",
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make prediction\n",
    "\n",
    "grid_dt.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our grids have been one-dimensional, now let's try using multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param grid with test different split criteria as well.\n",
    "param_grid_dt = {'min_samples_split' : range(2, 30 ,2), \"max_depth\": depths_range}\n",
    "\n",
    "param_grid_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "grid_dt = \n",
    "\n",
    "#Fit grid on data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter\n",
    "\n",
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n",
    "\n",
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many models did the grid search algorithm fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add another dimension\n",
    "param_grid_dt[\"max_features\"] = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "grid_dt = GridSearchCV(estimator = DecisionTreeClassifier(), \n",
    "                        param_grid = param_grid_dt, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "grid_dt.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter\n",
    "\n",
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n",
    "\n",
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously grid search takes a long time and in some case can cause memory errors. This is where RandomizedSearchCV comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize RandomizedSearchCV grid with n_iter = 15\n",
    "grid_dt = \n",
    "\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced run time by  a whole a lot. But now let's see if we sacrificed performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check best score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to using the KNN model.\n",
    "\n",
    "We know that we need to scale our data for the KNN algorithm right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data and fit it a Grid search function it.\n",
    "\n",
    "#Intialize scalar\n",
    "# scale = StandardScaler()\n",
    "\n",
    "#Fit and transform scaler on the data\n",
    "# Xs = scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass scaler and knn classifier objects into make_pipeline function\n",
    "pipe = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new param_grid\n",
    "neighbors_range = range(2, 21)\n",
    "param_grid_knn = {}\n",
    "param_grid_knn[\"kneighborsclassifier__n_neighbors\"] = neighbors_range\n",
    "param_grid_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass in pipe into GridSearchCV function, \n",
    "grid_knn_pipe = \n",
    "\n",
    "\n",
    "#Fit on original versions of data\n",
    "\n",
    "\n",
    "\n",
    "#Best scores and params\n",
    "print (grid_knn_pipe.best_score_, grid_knn_pipe.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cross-validation process using Pipeline\n",
    "pipe = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in our classification lessons we've mainly modeling binary classification datasets aka either-or data. In this class we're going to work through the MNIST digits dataset and learn to work with and interpret models trained on multiple classes—multiple meaning more than two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the MNIST dataset, let's bring back the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in iris data using seaborn\n",
    "iris = sb.load_dataset(\"iris\")\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset is a mulit-class dataset because there are three uniques values in the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The class of species\n",
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's model this data and using a confusion matrix to analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1. Assign X and y\n",
    "\n",
    "X = \n",
    "\n",
    "y = \n",
    "\n",
    "#Step 2. train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "\n",
    "#Step 3. Fit KNN model with 3 neighbors on training data\n",
    "\n",
    "knn = \n",
    "\n",
    "\n",
    "\n",
    "#Step 4. Make predictions on the X_test using the model\n",
    "\n",
    "preds = \n",
    "\n",
    "#Step 5. Score predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty decent model right? Now let's use the confusion matrix to better understand our predictions, particulary the wrong ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Call pass in y_test and preds into confusion_matrix object\n",
    "\n",
    "cm = \n",
    "\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column and index values for our confusion matrix dataframe\n",
    "cols = [\"pred_setosa\", \"pred_versicolor\", \"pred_virginica\"]\n",
    "index = [\"actual_setosa\", \"actual_versicolor\", \"actual_virginica\"]\n",
    "\n",
    "#Make dataframe out of confusion matrix\n",
    "\n",
    "cm_df = pd.DataFrame(cm, columns=cols, index=index)\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score told us that our model correctly classified 88% of the testing dataset.\n",
    "\n",
    "What does the confusion matrix tell us that the accuracy score doesn't? What is the accuracy if we ignore setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non setosa accuracy score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 13, 15, 16, represent the values that we correctly identified.\n",
    "\n",
    "The two 3 values represent the values that we incorrectly identified.\n",
    "\n",
    "<br>\n",
    "\n",
    "If we want to calculate the recall and precision scores then we need to designate one of the classes as true and the rest as false\n",
    "\n",
    "![ee](https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAykAAAAJDUzZGVlZGM0LTUyNWMtNDNjZi1hNjkxLTdlZjEzY2VmMmM4OQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = TP/TP+FP\n",
    "\n",
    "Recall = TP/TP+FN\n",
    "\n",
    "Good blog post to refresh your memory on these metrics:http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The recall score for setaosa\n",
    "\n",
    "tp_set = cm_df.loc[\"actual_setosa\", \"pred_setosa\"]\n",
    "\n",
    "#Divide tp_set by the sum of values in the actual setosa row\n",
    "\n",
    "tp_set/float(cm_df.loc[\"actual_setosa\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The precision score for setosa\n",
    "\n",
    "\n",
    "\n",
    "#Divide tp_set by the sum of values in the predicted setosa column\n",
    "\n",
    "tp_set/float(cm_df.pred_setosa.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got perfect scores but thats boring, let's try this with versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The recall score for versicolor\n",
    "\n",
    "tp_ver = cm_df.loc[\"actual_versicolor\", \"pred_versicolor\"]\n",
    "\n",
    "#Divide tp_set by the sum of values in the actual versicolor row\n",
    "\n",
    "tp_ver/float(cm_df.ix[\"actual_versicolor\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The precision score for versicolor\n",
    "\n",
    "#Divide tp_set by the sum of values in the predicted versicolor column\n",
    "\n",
    "tp_ver/float(cm_df.pred_versicolor.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-class confusion matrix explained:\n",
    "![s](https://3.bp.blogspot.com/-YpiS7AXxlgs/VEVrZGx5oaI/AAAAAAAAG1c/E8PdwoUamYw/s1600/multi-class-confusionmatrix.png)\n",
    "\n",
    "[Source](http://text-analytics101.rxnlp.com/2014/10/computing-precision-and-recall-for.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Famous machine learning dataset that is frequently used over and over again in machine learning courses.\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hand written digits of the mnist dataset.\n",
    "![a](https://kuanhoong.files.wordpress.com/2016/01/mnistdigits.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in dataset from sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_dict = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data description\n",
    "digits_dict[\"DESCR\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "\n",
    "data = digits_dict[\"data\"]\n",
    "\n",
    "target = digits_dict[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at target variable\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to show there are 1797 images (8 by 8 images for a dimensionality of 64)\n",
    "print('Image Data Shape' , digits_dict.data.shape)\n",
    "# Print to show there are 1797 labels (integers from 0–9)\n",
    "print(\"Label Data Shape\", digits_dict.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View data of single digit\n",
    "\n",
    "data[1].reshape(8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell what number this data is showing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use matplotlib to view the images\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(data[0:5], target[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "    plt.title('Training: %i\\n' % label, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a logistic regression algorithm to model this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Step 1. train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "\n",
    "#Step 2. Fit DecisionTreeClassifier with max_depth 10 on training data\n",
    "\n",
    "\n",
    "\n",
    "#Step 3. Make predictions on the X_test using the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Step 4. Score predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create confusion matrix from predictions and y_test\n",
    "\n",
    "cm_digits = \n",
    "\n",
    "cm_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap version\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sb.heatmap(cm_digits, annot=True, cmap= \"coolwarm\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"Actual Labels\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_mldata\n",
    "# mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some of the images we got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify indices of wrong predictions\n",
    "index = 0\n",
    "false_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_preds = y_test != preds\n",
    "\n",
    "for i, e in enumerate(false_preds):\n",
    "    if e: \n",
    "        false_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassifiedIndexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for plotIndex, badIndex in enumerate(false_index[0:5]):\n",
    "    plt.subplot(1, 5, plotIndex + 1)\n",
    "    plt.imshow(np.reshape(X_test[badIndex], (8,8)), cmap=plt.cm.gray)\n",
    "    plt.title(\"Predicted: {}, Actual: {}\".format(preds[badIndex], y_test[badIndex]), fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class exercise time: \n",
    "\n",
    "Which digit did our model do the best at identifying? What digit was the worst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score answer \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision score answer \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall score answer \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Class Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As if often the case in real-world machine learning, there is a huge imbalance between the distribution of class of the variable we're trying to predicting.\n",
    "\n",
    "Class False: 98.23%\n",
    "\n",
    "Class True: 1.77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrated example\n",
    "![www](http://www.svds.com/wp-content/uploads/2016/08/messy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem that imbalanced machine learning project present that is they render the accuracy score metric almost irrelevant.\n",
    "\n",
    "If our null accuracy is 99.5%, then we don't have much room for improvement.\n",
    "\n",
    "<br>\n",
    "\n",
    "Imagine a confusion with 1000 TNs, 20 FPs, 15 FNs, and 25 TPs. What would the accuracy, precision, and recall scores be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix form\n",
    "pd.DataFrame(data=[[1000, 20], [15, 25]], \n",
    "             columns=[\"Predicted_False\", \"Predicted_True\"], \n",
    "             index=[\"Actual_False\", \"Actual_True\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign variables\n",
    "\n",
    "tn = 1000.\n",
    "fp = 20.\n",
    "fn = 15.\n",
    "tp = 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score\n",
    "\n",
    "(tp + tn)/(tn + fp + fn + tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good score! Or is it???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whats the null accuracy\n",
    "\n",
    "(tn + fp)/(tn + fp + fn + tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hardly beat the null accuracy. Now let's calculate precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "\n",
    "tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall\n",
    "tp/(tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have a good model or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move onto the real thing by modeling credit card fraud data\n",
    "\n",
    "https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/fraud.csv\"\n",
    "\n",
    "fraud = pd.read_csv(path, index_col=[0])\n",
    "\n",
    "fraud.drop(\"Time\", axis = 1, inplace = True)\n",
    "fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The V features are principal components which we'll learn about next Thursday, in the meantime think of them as hidden features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'see how imbalanced the classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts without normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value counts with normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty imbalanced would you say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick EDA to find relationship amount and fraud status\n",
    "\n",
    "fraud.groupby(\"Class\").mean()[\"Amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this say about the relationship between amount of the transaction and fraudelent status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into modeling, what metric should we try to minimize False Negatives or False Positives? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some modeling.\n",
    "\n",
    "Train a logistic regression model and evaluate it on the testing dataset using accuracy, recall, and precision scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign variables\n",
    "\n",
    "X = fraud.drop(\"Class\", axis = 1)\n",
    "\n",
    "y = fraud.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit logistic regression model on training data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null accuracy of testing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate it on testing set using accuracy score\n",
    "\n",
    "preds = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate it on testing set using precision score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate it on testing set using recall score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these metrics tell us about our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validate using precision score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validate using precision score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validate using roc_auc score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive probabilities of class 1 from the test set\n",
    "\n",
    "\n",
    "#Pass in the test_probs variable and the true test labels aka y_test in the roc_curve function\n",
    "\n",
    "\n",
    "#Outputs the fpr, tpr, for varying thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC_curve again but this time annotate the curve with the threshold value\n",
    "plt.figure(figsize=(15,11))\n",
    "plt.plot(fpr, tpr, linewidth=12, alpha = .6)\n",
    "plt.plot([0,1], [0,1], \"--\", alpha=.6)\n",
    "for label, x, y in zip(thres[::10], fpr[::10], tpr[::10]):\n",
    "    plt.annotate(\"{0:.2f}\".format(label), xy=(x, y ), size = 15)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret this chart?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced class techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eop](https://chrisalbon.com/images/machine_learning_flashcards/Downsampling_print.png)\n",
    "Source: Chris Albon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead apply down sampling to our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many true class are there in the training dataset?\n",
    "\n",
    "fraud.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign number of fraud class in training data to variable\n",
    "N = fraud.Class.value_counts()[1]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import resample function from sklearn\n",
    "from sklearn.utils import resample\n",
    "\n",
    "fraud_maj = fraud[fraud.Class==0]\n",
    "fraud_min = fraud[fraud.Class==1]\n",
    " \n",
    "# Downsample majority class\n",
    "fraud_majority_downsampled = resample(fraud_maj, \n",
    "                                 replace=False,     # Do not sample with replacement\n",
    "                                 n_samples=N,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "fraud_ds = pd.concat([fraud_majority_downsampled, fraud_min])\n",
    " \n",
    "# Display new class counts\n",
    "fraud_ds.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfectly balanced classes. Let's use cross validate to see how well our model does.\n",
    "\n",
    "Use accuracy, recall, precision, and roc_auc metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ds = fraud_ds.drop(\"Class\", axis = 1)\n",
    "y_ds = fraud_ds.Class\n",
    "\n",
    "cross_val_score(LogisticRegression(), X_ds, y_ds, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "cross_val_score(LogisticRegression(), X_ds, y_ds, cv = 5, scoring=\"precision\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "cross_val_score(LogisticRegression(), X_ds, y_ds, cv = 5, scoring=\"recall\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n",
    "cross_val_score(LogisticRegression(), X_ds, y_ds, cv = 5, scoring=\"roc_auc\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us about our model and our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aw](https://chrisalbon.com/images/machine_learning_flashcards/Upsampling_print.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Number non fraud observations\n",
    "N = fraud.Class.value_counts()[0]\n",
    "\n",
    "# Downsample majority class\n",
    "fraud_minority_upsampled = resample(fraud_min, \n",
    "                                 replace=True,     # Do not sample with replacement\n",
    "                                 n_samples=N,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "fraud_us = pd.concat([fraud_minority_upsampled, fraud_maj])\n",
    " \n",
    "# Display new class counts\n",
    "fraud_us.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "X_us = fraud_us.drop(\"Class\", axis = 1)\n",
    "y_us = fraud_us.Class\n",
    "\n",
    "#Accuracy score\n",
    "cross_val_score(LogisticRegression(), X_us, y_us, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision score\n",
    "cross_val_score(LogisticRegression(), X_us, y_us, cv = 5, scoring=\"precision\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall\n",
    "cross_val_score(LogisticRegression(), X_us, y_us, cv = 5, scoring=\"recall\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roc auc score\n",
    "cross_val_score(LogisticRegression(), X_us, y_us, cv = 5, scoring=\"roc_auc\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both techniques, our interpretation of the accuracy score is more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wee](https://svds.com/wp-content/uploads/2016/08/ImbalancedClasses_fig5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is an issue here and that is can a model trained on balanced data work well with imbalanced data? Let's find out!\n",
    "\n",
    "\n",
    "We're going to train a logistic regression model on a downsampled training dataset and then apply it to an imbalanced testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reassign variables\n",
    "\n",
    "X = fraud.drop(\"Class\", axis = 1)\n",
    "\n",
    "y = fraud.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the two training datasets\n",
    "train = X_train.copy()\n",
    "train[\"Class\"] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class count\n",
    "train.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train.Class.value_counts()[1]\n",
    "\n",
    "fraud_maj = train[train.Class==0]\n",
    "fraud_min = train[train.Class==1]\n",
    " \n",
    "# Downsample majority class\n",
    "fraud_majority_downsampled = resample(fraud_maj, \n",
    "                                 replace=False,     # Do not sample with replacement\n",
    "                                 n_samples=N,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "fraud_ds = pd.concat([fraud_majority_downsampled, fraud_min])\n",
    " \n",
    "# Display new class counts\n",
    "fraud_ds.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train Logistic Regression on downsampled data and evaluate it on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "fraud_ds_X = fraud_ds.drop(\"Class\", axis = 1)\n",
    "fraud_ds_y = fraud_ds.Class\n",
    "#Intialize\n",
    "lr = LogisticRegression()\n",
    "lr.fit(fraud_ds_X, fraud_ds_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null accuracy \n",
    "\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on testing dataset\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision\n",
    "\n",
    "precision_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall\n",
    "\n",
    "recall_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's your interpretation now?\n",
    "\n",
    "<br>\n",
    "Let's the upsampling technique to see if that produces a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train.Class.value_counts()[0]\n",
    "\n",
    "fraud_maj = train[train.Class==0]\n",
    "fraud_min = train[train.Class==1]\n",
    " \n",
    "# Downsample majority class\n",
    "fraud_minority_upsampled = resample(fraud_min, \n",
    "                                 replace=True,     # Do not sample with replacement\n",
    "                                 n_samples=N,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "fraud_us = pd.concat([fraud_minority_upsampled, fraud_maj])\n",
    " \n",
    "# Display new class counts\n",
    "fraud_us.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "fraud_us_X = fraud_ds.drop(\"Class\", axis = 1)\n",
    "fraud_us_y = fraud_ds.Class\n",
    "#Intialize\n",
    "lr = LogisticRegression()\n",
    "lr.fit(fraud_us_X, fraud_us_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on testing dataset\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision\n",
    "\n",
    "precision_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall\n",
    "\n",
    "recall_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we make of these results??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the good news. We can set a class_weight setting in our models to be assigned to \"balanced\".\n",
    "\n",
    "From sklearn:\n",
    "\n",
    "\"The 'balanced' mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cross validate with Logistic Regression, Decision Trees, and RandomForest models with the class_weight parameter set to \"balanced.\"\n",
    "\n",
    "But first let's calculate those weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class 0 weight\n",
    "(y.shape[0])/float((2*y.value_counts()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class 1 weight\n",
    "(y.shape[0])/float((2*y.value_counts()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple class counts by weights\n",
    "64315*0.5038249242011972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "65*492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression \n",
    "\n",
    "cross_val_score(LogisticRegression(class_weight=\"balanced\"), \n",
    "                X, y, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree with max depth = 12\n",
    "\n",
    "cross_val_score(DecisionTreeClassifier(class_weight=\"balanced\", max_depth=14), \n",
    "                X, y, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest with n_estimators = 40\n",
    "\n",
    "cross_val_score(RandomForestClassifier(class_weight=\"balanced\", n_estimators=40), \n",
    "                X, y, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "MNIST:\n",
    "\n",
    "- https://github.com/grfiv/MNIST/blob/master/knn/knn.ipynb\n",
    "- https://github.com/monsta-hd/ml-mnist\n",
    "- https://www.youtube.com/watch?v=aZsZrkIgan0\n",
    "- http://joshmontague.com/posts/2016/mnist-scikit-learn/\n",
    "\n",
    "Multi-class:\n",
    "\n",
    "- https://gallery.cortanaintelligence.com/Competition/Tutorial-Competition-Iris-Multiclass-Classification-2\n",
    "-https://www.youtube.com/watch?v=6kzvrq-MIO0\n",
    "\n",
    "\n",
    "Imbalanced classes:\n",
    "\n",
    "- https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba\n",
    "- https://svds.com/learning-imbalanced-classes/\n",
    "- https://www.youtube.com/watch?v=X9MZtvvQDR4\n",
    "- https://elitedatascience.com/imbalanced-classes\n",
    "- https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "Grid Search and Pipelines\n",
    "\n",
    "- https://chrisalbon.com/machine-learning/cross_validation_parameter_tuning_grid_search.html\n",
    "- https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "- https://chrisalbon.com/machine-learning/pipelines_with_parameter_optimization.html\n",
    "- https://chrisalbon.com/machine-learning/hyperparameter_tuning_using_random_search.html\n",
    "- https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/\n",
    "- https://www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Gridsearch with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in boston dataset\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston[\"data\"])\n",
    "df.columns = boston[\"feature_names\"]\n",
    "df[\"MEDV\"] = boston[\"target\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = df.drop(\"MEDV\", axis =1)\n",
    "y = df.MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a Pipeline Class instead of function to establish pipeline\n",
    "pipe_poly = Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
    "                           ('linearregression', LinearRegression())])                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a few features from X\n",
    "XX = X[[\"RM\", \"DIS\", \"NOX\", \"CRIM\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize range values for poly\n",
    "poly_range = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "#Intialize grid dictionary\n",
    "param_grid_poly = {}\n",
    "\n",
    "#Input grid values\n",
    "param_grid_poly[\"polynomialfeatures__degree\"] = poly_range\n",
    "\n",
    "#Establish the grid\n",
    "poly_grid = GridSearchCV(pipe_poly, \n",
    "                         param_grid = param_grid_poly, cv=5, \n",
    "                         scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_grid.fit(XX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_poly = Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
    "                            ('ridgeregression', Ridge())]) \n",
    "\n",
    "param_grid_ridge = {'polynomialfeatures__degree': [1, 2, 3, 4, 5],\n",
    "              'ridgeregression__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_ridge = RandomizedSearchCV(pipe_poly, param_distributions=param_grid_ridge, \n",
    "                                n_iter = 5 , cv = 5, scoring='neg_mean_squared_error')\n",
    "grid_ridge.fit(XX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (grid_ridge.best_params_, grid_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
