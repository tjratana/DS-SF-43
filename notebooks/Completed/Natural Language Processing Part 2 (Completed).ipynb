{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to pick up where we left off\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- Finish text classification lesson by using stemming and lemmatization in our vectorizers\n",
    "- Build a simple text summarizer\n",
    "- How to find similar documents with cosine similarity and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from time import time\n",
    "import pandas as pd\n",
    "pd.set_option(\"max.colwidth\", 500)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap our text classification section, we're going to learn how to incorporate stemming and lemmatization in our vectorizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ing...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\r\\n\\r\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We we...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I also dig their candy selection :)</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\r\\n\\r\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\r\\n\\r\\nThe fenced in area is huge to let the dogs run, play, and sniff!</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\\r\\n\\r\\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ing...   \n",
       "1  I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\r\\n\\r\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We we...   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                         love the gyro plate. Rice is so good and I also dig their candy selection :)   \n",
       "3                                                                      Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\r\\n\\r\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\r\\n\\r\\nThe fenced in area is huge to let the dogs run, play, and sniff!   \n",
       "4                          General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\\r\\n\\r\\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)   \n",
       "\n",
       "     type                 user_id  cool  useful  funny  \n",
       "0  review  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  review  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  review  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  review  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  review  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in yelp review data\n",
    "\n",
    "path = \"../../data/NLP_data/yelp.csv\"\n",
    "\n",
    "yelp = pd.read_csv(path, encoding='unicode-escape')\n",
    "\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame called yelp_best_worst that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-e94571642a03>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-e94571642a03>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print y.value_counts(normalize=True)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "#Null accuracy\n",
    "print y.value_counts(normalize=True)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the analyzer section of the CountVectorizer doc strings\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzer argument allows us to upload our function to transform/tokenize the words in our corpura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of stems\n",
    "def word_tokenize_stem(text):\n",
    "    #Transform and tokenize words using TextBlob\n",
    "    words = TextBlob(text).words\n",
    "    #Intialize stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    #Return a list of the stems\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "\n",
    "# define a function that accepts text and returns a list of lemons (noun version)\n",
    "def word_tokenize_lemma(text):\n",
    "    #Transform and tokenize words using TextBlob\n",
    "    words = TextBlob(text).words\n",
    "    #Return a list of lemons\n",
    "    return [word.lemmatize() for word in words]\n",
    "\n",
    "# define a function that accepts text and returns a list of lemons (verb version)\n",
    "def word_tokenize_lemma_verb(text):\n",
    "    words = TextBlob(text).words\n",
    "    #Return a list of lemons    \n",
    "    return [word.lemmatize(pos=\"v\") for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our three new functions with both count and tfidf vectorizers. \n",
    "<br>\n",
    "- First let's create a function that takes in an initialized but unfit vectorizer as an argument.\n",
    "- Fit and transforms training data using the vectorizer\n",
    "- Transforms the testing data\n",
    "- Fits naive bayes model on training data.\n",
    "- Evaluate it on the training and testing data.\n",
    "- Prints the number of features and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_model_evaluator(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    print (\"Features: \", X_train_dtm.shape[1])\n",
    "    print (\"Training Score: \", nb.score(X_train_dtm, y_train))\n",
    "    print (\"Testing Score: \", nb.score(X_test_dtm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13273\n",
      "Training Score:  0.970626631854\n",
      "Testing Score:  0.924657534247\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_stem\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_stem)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.974216710183\n",
      "Testing Score:  0.904109589041\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  19431\n",
      "Training Score:  0.974216710183\n",
      "Testing Score:  0.906066536204\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma_verb\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma_verb)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret these results? Let's try it again with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13273\n",
      "Training Score:  0.816906005222\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_stem\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_stem)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.817232375979\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.817232375979\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the tfidf vectorizers compare to counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search time. Let's grid search objects that incorporate all of the analyzer functions for count and tfidf vectorizers. In addition we'll do the same for randomized search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make pipeline for countvectorizer and naive bayes model\n",
    "pipe_cv = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#Intialize parameters for count vectorizer\n",
    "param_grid_cv = {}\n",
    "param_grid_cv[\"countvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_cv[\"countvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_cv[\"countvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__binary\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search object\n",
    "\n",
    "grid_cv = GridSearchCV(pipe_cv, param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_cv.fit(X, y)\n",
    "#Print time elapsed\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters\n",
    "print (grid_cv.best_params_)\n",
    "#Best score\n",
    "print (grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidfvectorizer gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make pipeline for tfidfvectorizer and naive bayes model\n",
    "pipe_tf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "\n",
    "#Intialize parameters for tfidf vectorizer\n",
    "param_grid_tf = {}\n",
    "param_grid_tf[\"tfidfvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_tf[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_tf[\"tfidfvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__binary\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search object\n",
    "\n",
    "grid_tf = GridSearchCV(pipe_tf, param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_tf.fit(X, y)\n",
    "#Print time elapsed\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282.130045176\n"
     ]
    }
   ],
   "source": [
    "#Randomized grid search with n_iter = 10\n",
    "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 10,\n",
    "                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_cv.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'countvectorizer__lowercase': True, 'countvectorizer__analyzer': 'word', 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__binary': False, 'countvectorizer__max_features': 7500}\n",
      "0.931962799804\n"
     ]
    }
   ],
   "source": [
    "#Best params\n",
    "print (randsearch_cv.best_params_)\n",
    "#Best score\n",
    "print (randsearch_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidfvectorizer randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomized grid search with n_iter = 10\n",
    "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n",
    "                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_tf.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best params\n",
    "print (randsearch_tf.best_params_)\n",
    "#Best score\n",
    "print (randsearch_tf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wraps up text classification. Now onto the rest of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing text\n",
    "\n",
    "We're going to build a very simple summarizer that uses tfidf scores on a corpura of data science and artificial intelligence articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the greatest difficulties that companies wishing to become more analytical have encountered over the last several years is finding good analysts and data scientists. A considerable amount of printer’s ink has been spilled into articles over this issue. Many of them mention consultants’ or analyst firms’ projections about how many quantitative analysts or data scientists will be needed in our society and conclude that it will be incredibly difficult to find them.\\n\\nI always thought th...</td>\n",
       "      <td>What Data Scientist Shortage? Get Serious and Get Talent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Within soccer’s nascent analytics movement, one metric dominates most discussions. It’s called Expected Goals or xG. Models for calculating xG differ, but the underlying concept is the same. In a nutshell, xG takes a shot’s characteristics – distance from goal, angle from goal, root cause, etc. – and assigns a probability that said shot will result in a goal. Accounting for these probabilities reveals which team creates better scoring opportunities. Given a season of data, xG analysis is a p...</td>\n",
       "      <td>xG, Soccer Analytics of Bundesliga in R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The company’s adjacent market opportunities are growing at a CAGR of 18% for the next five years.\\n\\nQualcomm (NASDAQ: QCOM) announced a few days ago that its subsidiary Qualcomm Technologies will offer OEMs its first machine learning SDK for running their own neural network models on devices powered by Snapdragon 820 SoCs. The devices include smartphones, cars and drones among many others. Gary Brotman, director of product management, Qualcomm Technologies, said:\\n\\nWith the introduction of...</td>\n",
       "      <td>Qualcomm: Taking Artificial Intelligence To A New Level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Web, Tech Companies Use GPUs to Put Deep Learning at Your Fingertips\\n\\nGPUs have helped researchers spark a deep-learning revolution that’s given computers super-human capabilities.\\n\\nThey’ve already enabled breakthrough results on the industry-standard ImageNet benchmark. They’re powering Facebook’s “Big Sur” deep learning computing platform. They’re also accelerating major advances in deep learning across a broad range of fields.\\n\\nGPUs have become the go-to technology for training ...</td>\n",
       "      <td>How Companies Use GPUs to Put Deep Learning at Your Fingertips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White House technology policy adviser Kristen Honey urged government and industry IT leaders to support the open data movement and showcase their work at two upcoming data innovation events.\\n\\nSpeaking Wednesday to a standing-room-only audience at the annual Data Innovation Summit in Washington, Honey highlighted a number of the administration’s open data initiatives, dating back to 2009, that are leading to innovative advances in medicine, agriculture, energy, transportation and education....</td>\n",
       "      <td>White House official urges IT leaders to join open data efforts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  One of the greatest difficulties that companies wishing to become more analytical have encountered over the last several years is finding good analysts and data scientists. A considerable amount of printer’s ink has been spilled into articles over this issue. Many of them mention consultants’ or analyst firms’ projections about how many quantitative analysts or data scientists will be needed in our society and conclude that it will be incredibly difficult to find them.\\n\\nI always thought th...   \n",
       "1  Within soccer’s nascent analytics movement, one metric dominates most discussions. It’s called Expected Goals or xG. Models for calculating xG differ, but the underlying concept is the same. In a nutshell, xG takes a shot’s characteristics – distance from goal, angle from goal, root cause, etc. – and assigns a probability that said shot will result in a goal. Accounting for these probabilities reveals which team creates better scoring opportunities. Given a season of data, xG analysis is a p...   \n",
       "2  The company’s adjacent market opportunities are growing at a CAGR of 18% for the next five years.\\n\\nQualcomm (NASDAQ: QCOM) announced a few days ago that its subsidiary Qualcomm Technologies will offer OEMs its first machine learning SDK for running their own neural network models on devices powered by Snapdragon 820 SoCs. The devices include smartphones, cars and drones among many others. Gary Brotman, director of product management, Qualcomm Technologies, said:\\n\\nWith the introduction of...   \n",
       "3  How Web, Tech Companies Use GPUs to Put Deep Learning at Your Fingertips\\n\\nGPUs have helped researchers spark a deep-learning revolution that’s given computers super-human capabilities.\\n\\nThey’ve already enabled breakthrough results on the industry-standard ImageNet benchmark. They’re powering Facebook’s “Big Sur” deep learning computing platform. They’re also accelerating major advances in deep learning across a broad range of fields.\\n\\nGPUs have become the go-to technology for training ...   \n",
       "4  White House technology policy adviser Kristen Honey urged government and industry IT leaders to support the open data movement and showcase their work at two upcoming data innovation events.\\n\\nSpeaking Wednesday to a standing-room-only audience at the annual Data Innovation Summit in Washington, Honey highlighted a number of the administration’s open data initiatives, dating back to 2009, that are leading to innovative advances in medicine, agriculture, energy, transportation and education....   \n",
       "\n",
       "                                                             title  \n",
       "0         What Data Scientist Shortage? Get Serious and Get Talent  \n",
       "1                          xG, Soccer Analytics of Bundesliga in R  \n",
       "2          Qualcomm: Taking Artificial Intelligence To A New Level  \n",
       "3   How Companies Use GPUs to Put Deep Learning at Your Fingertips  \n",
       "4  White House official urges IT leaders to join open data efforts  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in data\n",
    "\n",
    "path = \"../../data/NLP_data/ds_articles.csv\"\n",
    "\n",
    "#We're only be using the text and title columns\n",
    "articles = pd.read_csv(path, usecols=[\"text\", \"title\"], encoding=\"utf-8\")\n",
    "\n",
    "#Drop nulls\n",
    "articles.dropna(inplace=True)\n",
    "\n",
    "#Reset index\n",
    "articles.reset_index(inplace=True, drop=True)\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1418 entries, 0 to 1417\n",
      "Data columns (total 2 columns):\n",
      "text     1418 non-null object\n",
      "title    1418 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Info\n",
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "#Intialize tfidf with stop_words = english, max_features = 1000, and stem analzyer \n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=sw,max_df = 0.3,min_df=.05,\n",
    "                        analyzer=word_tokenize_lemma,)\n",
    "\n",
    "#Fit and transform the text using the tfidf vectorizer\n",
    "text = articles.text\n",
    "dtm = tfidf.fit_transform(text)\n",
    "\n",
    "#Assign tokens to features\n",
    "features = tfidf.get_feature_names()\n",
    "\n",
    "print (len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of features and their idf scores\n",
    "idfscores = pd.DataFrame()\n",
    "idfscores[\"tokens\"] = features\n",
    "idfscores[\"scores\"] = tfidf.idf_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>fail</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>challenging</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>road</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>camera</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>analyzed</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>stock</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>established</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>60</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>numerous</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>bringing</td>\n",
       "      <td>3.981042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tokens    scores\n",
       "584          fail  3.981042\n",
       "322   challenging  3.981042\n",
       "1108         road  3.981042\n",
       "303        camera  3.981042\n",
       "209      analyzed  3.981042\n",
       "1220        stock  3.981042\n",
       "548   established  3.981042\n",
       "23             60  3.981042\n",
       "905      numerous  3.981042\n",
       "292      bringing  3.981042"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten most imporant words\n",
    "idfscores.sort_values(by=\"scores\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>large</td>\n",
       "      <td>2.207974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>across</td>\n",
       "      <td>2.207974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Google</td>\n",
       "      <td>2.210335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>available</td>\n",
       "      <td>2.215075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>possible</td>\n",
       "      <td>2.215075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>day</td>\n",
       "      <td>2.215075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>experience</td>\n",
       "      <td>2.222226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>They</td>\n",
       "      <td>2.222226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>think</td>\n",
       "      <td>2.222226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>type</td>\n",
       "      <td>2.224621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokens    scores\n",
       "778        large  2.207974\n",
       "164       across  2.207974\n",
       "70        Google  2.210335\n",
       "248    available  2.215075\n",
       "988     possible  2.215075\n",
       "432          day  2.215075\n",
       "568   experience  2.222226\n",
       "131         They  2.222226\n",
       "1267       think  2.222226\n",
       "1308        type  2.224621"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top ten least imporant words\n",
    "idfscores.sort_values(by=\"scores\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's our summarizer function that will randomly select an article to summarize. By summarize, I mean show the top five words with the highest tfidf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    #Randomly choose index value\n",
    "    index = np.random.choice(articles.index, 1)[0]\n",
    "    article = text.iloc[index]\n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(article).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[index, features.index(word)]\n",
    "            \n",
    "   # print words with the top 5 TF-IDF scores\n",
    "    print ('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print (word)\n",
    "        \n",
    "    #Print title of article\n",
    "    print (\"\\n\", articles.title[index])\n",
    "    \n",
    "    #Print the text of article\n",
    "#     print article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "emerging\n",
      "risk\n",
      "cluster\n",
      "claim\n",
      "approach\n",
      "\n",
      " Key tools of Big Data for Transformation: Review & Case Study\n"
     ]
    }
   ],
   "source": [
    "#Give it a go\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity with Cosine Similarity and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "![ew](https://i2.wp.com/dataaspirant.com/wp-content/uploads/2015/04/cosine.png?w=697)\n",
    "<br><br>\n",
    "\" Cosine similarity metric finds the normalized dot product of the two attributes. By determining the cosine similarity, we would effectively try to find the cosine of the angle between the two objects. The cosine of 0° is 1, and it is less than 1 for any other angle.\n",
    "\n",
    "It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "\n",
    "Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in (0,1). One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors.\"\n",
    "<br>\n",
    "Source: [Dataaspirant](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Diy cosine similarity function\n",
    "\n",
    "def square_rooted(x):\n",
    "\n",
    "    return round(np.sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def cosine_similarity_function(x,y):\n",
    "\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator),3)\n",
    " \n",
    "vec1 = [3, 45, 7, 2]\n",
    "vec2 = [2, 54, 13, 15]\n",
    "cosine_similarity_function(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive matrix of similarities between all the data science articles documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate cosine distance for each pair of documents\n",
    "dist = cosine_similarity(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1408</th>\n",
       "      <th>1409</th>\n",
       "      <th>1410</th>\n",
       "      <th>1411</th>\n",
       "      <th>1412</th>\n",
       "      <th>1413</th>\n",
       "      <th>1414</th>\n",
       "      <th>1415</th>\n",
       "      <th>1416</th>\n",
       "      <th>1417</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113929</td>\n",
       "      <td>0.070045</td>\n",
       "      <td>0.079961</td>\n",
       "      <td>0.116752</td>\n",
       "      <td>0.154060</td>\n",
       "      <td>0.112959</td>\n",
       "      <td>0.060669</td>\n",
       "      <td>0.084572</td>\n",
       "      <td>0.322649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164872</td>\n",
       "      <td>0.142885</td>\n",
       "      <td>0.095520</td>\n",
       "      <td>0.194064</td>\n",
       "      <td>0.176607</td>\n",
       "      <td>0.086310</td>\n",
       "      <td>0.153051</td>\n",
       "      <td>0.025908</td>\n",
       "      <td>0.193765</td>\n",
       "      <td>0.138436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076533</td>\n",
       "      <td>0.047124</td>\n",
       "      <td>0.058152</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>0.068518</td>\n",
       "      <td>0.036861</td>\n",
       "      <td>0.038336</td>\n",
       "      <td>0.083151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108322</td>\n",
       "      <td>0.096620</td>\n",
       "      <td>0.079264</td>\n",
       "      <td>0.088891</td>\n",
       "      <td>0.094266</td>\n",
       "      <td>0.090885</td>\n",
       "      <td>0.050130</td>\n",
       "      <td>0.021027</td>\n",
       "      <td>0.055269</td>\n",
       "      <td>0.041649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.070045</td>\n",
       "      <td>0.076533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102324</td>\n",
       "      <td>0.078817</td>\n",
       "      <td>0.097378</td>\n",
       "      <td>0.217310</td>\n",
       "      <td>0.049943</td>\n",
       "      <td>0.149809</td>\n",
       "      <td>0.131007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198051</td>\n",
       "      <td>0.024482</td>\n",
       "      <td>0.051428</td>\n",
       "      <td>0.094567</td>\n",
       "      <td>0.093622</td>\n",
       "      <td>0.074362</td>\n",
       "      <td>0.065025</td>\n",
       "      <td>0.146032</td>\n",
       "      <td>0.201584</td>\n",
       "      <td>0.073423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.079961</td>\n",
       "      <td>0.047124</td>\n",
       "      <td>0.102324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038511</td>\n",
       "      <td>0.149229</td>\n",
       "      <td>0.035401</td>\n",
       "      <td>0.049433</td>\n",
       "      <td>0.140992</td>\n",
       "      <td>0.079774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.073937</td>\n",
       "      <td>0.150466</td>\n",
       "      <td>0.128223</td>\n",
       "      <td>0.100471</td>\n",
       "      <td>0.083198</td>\n",
       "      <td>0.169101</td>\n",
       "      <td>0.054842</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.037017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.116752</td>\n",
       "      <td>0.058152</td>\n",
       "      <td>0.078817</td>\n",
       "      <td>0.038511</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048237</td>\n",
       "      <td>0.087655</td>\n",
       "      <td>0.110262</td>\n",
       "      <td>0.034083</td>\n",
       "      <td>0.122892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043802</td>\n",
       "      <td>0.057226</td>\n",
       "      <td>0.036450</td>\n",
       "      <td>0.062074</td>\n",
       "      <td>0.142089</td>\n",
       "      <td>0.114308</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>0.079380</td>\n",
       "      <td>0.045385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1418 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  1.000000  0.113929  0.070045  0.079961  0.116752  0.154060  0.112959   \n",
       "1  0.113929  1.000000  0.076533  0.047124  0.058152  0.077700  0.068518   \n",
       "2  0.070045  0.076533  1.000000  0.102324  0.078817  0.097378  0.217310   \n",
       "3  0.079961  0.047124  0.102324  1.000000  0.038511  0.149229  0.035401   \n",
       "4  0.116752  0.058152  0.078817  0.038511  1.000000  0.048237  0.087655   \n",
       "\n",
       "       7         8         9       ...         1408      1409      1410  \\\n",
       "0  0.060669  0.084572  0.322649    ...     0.164872  0.142885  0.095520   \n",
       "1  0.036861  0.038336  0.083151    ...     0.108322  0.096620  0.079264   \n",
       "2  0.049943  0.149809  0.131007    ...     0.198051  0.024482  0.051428   \n",
       "3  0.049433  0.140992  0.079774    ...     0.045000  0.073937  0.150466   \n",
       "4  0.110262  0.034083  0.122892    ...     0.043802  0.057226  0.036450   \n",
       "\n",
       "       1411      1412      1413      1414      1415      1416      1417  \n",
       "0  0.194064  0.176607  0.086310  0.153051  0.025908  0.193765  0.138436  \n",
       "1  0.088891  0.094266  0.090885  0.050130  0.021027  0.055269  0.041649  \n",
       "2  0.094567  0.093622  0.074362  0.065025  0.146032  0.201584  0.073423  \n",
       "3  0.128223  0.100471  0.083198  0.169101  0.054842  0.089900  0.037017  \n",
       "4  0.062074  0.142089  0.114308  0.021850  0.025359  0.079380  0.045385  \n",
       "\n",
       "[5 rows x 1418 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make it a dataframe\n",
    "dist_df = pd.DataFrame(dist)\n",
    "\n",
    "#Shape\n",
    "dist_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare some articles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index position of article\n",
    "index = 239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Popular TV Shows on Data Science and Artificial Intelligence\n",
      "\n",
      ", ************************************************ \n",
      " Introduction\n",
      "\n",
      "The development of full artificial intelligence could spell the end of human race. – Stephen Hawking\n",
      "\n",
      "The world is now rapidly moving towards achieving this finest technology breakthrough ever. It is expected that AI would enrich humans with more power and opportunities. Another group of people (including Stephen Hawking and Elon Musk) believe that this might lead to human destruction (if not handled carefully).\n",
      "\n",
      "I think, it’s too early for us to envisage such uncertain future. Good news is, companies like Google, Microsoft, Baidu have already started creating products based on AI. It won’t be long enough to experience the influence of AI in our daily lives.\n",
      "\n",
      "Accidentally, my exploration of AI started with movie ‘Her’. The influence was so powerful that I ended up creating an infographic on 10 Movies on Data Science and Machine Learning. May be a ~ 2 hours movie didn’t nourish my appetite well.\n",
      "\n",
      "Few days later, I came across a TV Series ‘Intelligence’. So far, this is one of the best show I’ve ever watched. Later, I found many other shows which beautifully describes a future world driven by data, technology, numbers and artificial intelligence. Here I’ve shared the best of them.\n",
      "\n",
      "Below are 10 Popular TV Shows on Data Science and Artificial Intelligence ( in no particular order). If you haven’t seen any of them, I’d suggest you to begin with ‘Intelligence’.\n",
      "\n",
      "If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.\n"
     ]
    }
   ],
   "source": [
    "#Assign titles column to titles variable\n",
    "\n",
    "titles = articles.title\n",
    "\n",
    "\n",
    "\n",
    "#Print title\n",
    "print (titles[index])\n",
    "\n",
    "#print article\n",
    "\n",
    "print (\"\\n, ************************************************ \\n\", text[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take the index value and use it grab the column of the scores between every article and the one at index 935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass\n",
    "dist_column = dist_df[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the index values of the 5 \n",
    "\n",
    "closest_index = dist_column.nlargest(6).index[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Must Watch Movies on Data Science and Machine Learning\n",
      "10 Must Watch Movies on Data Science and Machine Learning\n",
      "Why algorithms will be at the core of our AI-powered future, and why you should care\n",
      "Artificial Intelligence in business to deliver positive experiences\n",
      "Netflix Changed Its Rating System for AI Purposes\n"
     ]
    }
   ],
   "source": [
    "#Pass index values into titles and print them\n",
    "\n",
    "for i in titles.iloc[closest_index].tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1116    Introduction\\n\\nSome members of our team (including me) live by just 2 passions in life – Data Science & Movies! For us, slicing and dicing movies over Monday morning coffee is part of warming up ritual.\\n\\nSo, we decided to do a poll among ourselves on the best movies related to data science and machine learning. We also thought that we would release the outcome of the results in form of an infographic.\\n\\nNeedless to say there were heated debates and a few disappointed faces in our office!...\n",
       "1157    Introduction\\n\\nSome members of our team (including me) live by just 2 passions in life – Data Science & Movies! For us, slicing and dicing movies over Monday morning coffee is part of warming up ritual.\\n\\nSo, we decided to do a poll among ourselves on the best movies related to data science and machine learning. We also thought that we would release the outcome of the results in form of an infographic.\\n\\nNeedless to say there were heated debates and a few disappointed faces in our office!...\n",
       "1000    Lolita Taub\\n\\nBy Samantha Walravens & Heather Cabot\\n\\nIn 2011, entrepreneur and investor Marc Andreessen wrote his famous ,\"Why Software Is Eating the World\" in the Wall Street Journal. Today, that story would more likely read, \"Why Artificial Intelligence Is Eating the World.\" The market for artificial intelligence (AI) technologies-- from voice and image recognition to chat bots to self-driving cars-- is hot. A Narrative Science survey found last year that 38% of enterprises are already ...\n",
       "1130    For the second year in a row, We Are Social had the privilege of presenting at Vivid Sydney this year. Already one of the world’s leading festivals, Vivid is a bit like SXSW: an amalgam of inspiring people, creative work, and fresh ideas across a variety of themes and topics.\\n\\nFor our 2017 keynote, I joined forces with We Are Social’s Sydney MD, Suzie Shaw, to explore the impact that algorithms and machine learning are having on every aspect of our lives, and what that means for marketing....\n",
       "1009    The Non-Technical Guide to Machine Learning & Artificial Intelligence\\n\\nI have a challenge for you.\\n\\nIn a few seconds, I want you to stop reading this article, and follow the instructions below.\\n\\nHere you go:\\n\\n2. Hit “crtl + f”\\n\\n3. Search “artificial intelligence” or “machine learning”\\n\\n4. If you don't find any matches, publicly shame me on Twitter.\\n\\nI hope my point is obvious.\\n\\nMachine learning and artificial intelligence (ML and AI) have seized Tech mindshare in a way few to...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass index values into titles and but don't print\n",
    "text.iloc[closest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "It is standard practice to cluster with tfidf data instead of the count vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize clustering algorithm with 4 clusters and fit it on dtm\n",
    "\n",
    "km4 = KMeans(n_clusters=4)\n",
    "#Fit algorithm\n",
    "km4.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015203943655105008"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out silhouette score\n",
    "silhouette_score(dtm, km4.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign labels to articles dataframe \n",
    "\n",
    "articles[\"cluster\"] = km4.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 5 randomly selected headlines from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which machine learning algorithm should I use?\n",
      "Research Blog: Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System\n",
      "Bayesian Statistics Explained in Simple English For Beginners\n",
      "Eight (not 10) things an R user will find frustrating when trying to learn Python\n",
      "May the Force of R be With You, Always!\n"
     ]
    }
   ],
   "source": [
    "#Cluster 0\n",
    "for i in articles[articles.cluster == 0].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM Watson IoT and Its Integration with Blockchain\n",
      "A methodology for solving problems with DataScience for Internet of Things\n",
      "Detailed Overview – symbIoTe\n",
      "IoT’s killer app is home security\n",
      "Samsung Will Invest $1.2 Billion Into US For 'Internet Of Things'\n"
     ]
    }
   ],
   "source": [
    "#Cluster 1\n",
    "for i in articles[articles.cluster == 1].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany most likely to win Euro 2016\n",
      "Google's TensorFlow opens its AI doors to iOS developers\n",
      "Jerry Kaplan: \"Making Machine Learning Great Again\" | Talks at Google\n",
      "Rise of the robots: What advances mean for workers\n",
      "10 Real-World Examples Of Machine Learning And AI You Can Use Today\n"
     ]
    }
   ],
   "source": [
    "#Cluster 2\n",
    "for i in articles[articles.cluster == 2].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silicon Valley's Women In Data Science Creating New Opportunities For All Women In Startups And STEM\n",
      "How AI and machine learning can help solve IT's data management problem\n",
      "How mixed reality and machine learning are driving innovation in farming\n",
      "Real-time drives AI uptake\n",
      "How will AI shape the workforce of the future?\n"
     ]
    }
   ],
   "source": [
    "#Cluster 3\n",
    "for i in articles[articles.cluster == 3].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think the clusters are? Is it easy decipher? Ignore the silhouette score, does it pass the eye test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the top words of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " R\n",
      " Python\n",
      " Spark\n",
      " function\n",
      " variable\n",
      " code\n",
      " 2\n",
      " x\n",
      " class\n",
      " Learning\n",
      "\n",
      "\n",
      "Cluster 1:\n",
      " IoT\n",
      " device\n",
      " Internet\n",
      " Things\n",
      " sensor\n",
      " connected\n",
      " security\n",
      " 's\n",
      " platform\n",
      " home\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      " 's\n",
      " Google\n",
      " —\n",
      " said\n",
      " deep\n",
      " neural\n",
      " image\n",
      " he\n",
      " –\n",
      " researcher\n",
      "\n",
      "\n",
      "Cluster 3:\n",
      " analytics\n",
      " –\n",
      " scientist\n",
      " organization\n",
      " job\n",
      " platform\n",
      " insight\n",
      " marketing\n",
      " Big\n",
      " skill\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km4.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf.get_feature_names()\n",
    "for i in range(4):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this exercise again but this time we'll cluster the cosine distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize clustering algorithm with 4 clusters\n",
    "km4 = KMeans(n_clusters=4)\n",
    "\n",
    "#fit it on dist array\n",
    "\n",
    "km4.fit(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09926344148978795"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out silhouette score\n",
    "silhouette_score(dist, km4.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 5 randomly selected headlines from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign new labels to data frame\n",
    "\n",
    "articles[\"cluster_dist\"] = km4.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four Artificial Intelligence Challenges Facing The Industrial IoT\n",
      "10 Real World Applications of Internet of Things (IoT) – Explained in Videos\n",
      "Machine learning is leading the way to a smarter internet of things\n",
      "Implementation of Machine Learning in IoT - Tech-Talk by Aashish Kalra\n",
      "Does IoT Need Machine Learning To Succeed?\n"
     ]
    }
   ],
   "source": [
    "#Cluster 0\n",
    "for i in articles[articles.cluster_dist == 0].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Some of the Top Manufacturing Companies are Using AI\n",
      "The Tech Predictions for 2017 Series\n",
      "How AI will disrupt the classroom\n",
      "Turbocharge productivity and efficiency with AI\n",
      "The world told 6 guys no one would buy their software: Four years later they have a $940 million company\n"
     ]
    }
   ],
   "source": [
    "#Cluster 1\n",
    "for i in articles[articles.cluster_dist == 1].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbots and The Future of Marketing\n",
      "Intel Uses Chips Acquired in Deal With Startup to Drive Deep Learning\n",
      "Governments are the Tip of the Open Data Iceberg – Datazar Blog\n",
      "What Is The State Of Artificial Intelligence In China?\n",
      "The big data explosion sets us profound challenges - how can we keep up?\n"
     ]
    }
   ],
   "source": [
    "#Cluster 2\n",
    "for i in articles[articles.cluster_dist == 2].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series Prediction With Deep Learning in Keras\n",
      "What Are The Differences Between AI, Machine Learning, NLP, And Deep Learning?\n",
      "The Semantic Representation of Pure Mathematics—Wolfram Blog\n",
      "The 10 Algorithms Machine Learning Engineers Need to Know\n",
      "How do Chatbots work? A Guide to the Chatbot Architecture\n"
     ]
    }
   ],
   "source": [
    "#Cluster 3\n",
    "for i in articles[articles.cluster_dist == 3].sample(n=5).title.tolist():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "\n",
    "My fake news classifer article: https://opendatascience.com/blog/how-to-build-a-fake-news-classification-model/\n",
    "<br>\n",
    "My data science topic modeling article: https://opendatascience.com/blog/how-to-analyze-articles-about-data-science-using-data-science/\n",
    "<br><br>\n",
    "**Regular Expressions**\n",
    "- https://www.dataquest.io/blog/regular-expressions-data-scientists/\n",
    "- https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial\n",
    "- https://www.oreilly.com/ideas/an-introduction-to-regular-expressions\n",
    "\n",
    "\n",
    "**NLP Tutorials**\n",
    "\n",
    "- https://github.com/bonzanini/nlp-tutorial\n",
    "- https://github.com/totalgood/pycon-2016-nlp-tutorial\n",
    "\n",
    "**Text similarity:**\n",
    "- https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/\n",
    "- http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/\n",
    "- http://billchambers.me/tutorials/2014/12/22/cosine-similarity-explained-in-python.html\n",
    "- Explains why text similarity uses cosine similarity -> https://www.quora.com/What-are-the-mechanics-of-cosine-similarity-in-natural-language-processing\n",
    "\n",
    "**Text classification:**\n",
    "- Another fake news tutorial - > https://www.datacamp.com/community/tutorials/scikit-learn-fake-news\n",
    "- http://nlpforhackers.io/text-classification/\n",
    "- http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "- https://github.com/javedsha/text-classification\n",
    "- https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "- https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html\n",
    "\n",
    "\n",
    "**Text clustering:**\n",
    "\n",
    "- Great tutorial -> http://brandonrose.org/clustering\n",
    "- http://nlpforhackers.io/recipe-text-clustering/\n",
    "- https://pythonprogramminglanguage.com/kmeans-text-clustering/\n",
    "- http://mccormickml.com/2015/08/05/document-clustering-example-in-scikit-learn/\n",
    "\n",
    "\n",
    "**Word Embeddings/Word2Vec**\n",
    "\n",
    "- https://chatbotsmagazine.com/introduction-to-word-embeddings-55734fd7068a\n",
    "- https://www.springboard.com/blog/introduction-word-embeddings/\n",
    "- http://ruder.io/word-embeddings-1/\n",
    "- https://www.slideshare.net/BhaskarMitra3/a-simple-introduction-to-word-embeddings\n",
    "- https://github.com/fastai/word-embeddings-workshop\n",
    "\n",
    "\n",
    "**Topic Modeling**\n",
    "\n",
    "- http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "- https://blog.bigml.com/2016/11/16/introduction-to-topic-models/\n",
    "- http://nbviewer.jupyter.org/github/ogrisel/notebooks/blob/master/nmf_topics.ipynb?create=1\n",
    "- https://www.youtube.com/watch?v=ZgyA1Q2ywbM\n",
    "- https://www.youtube.com/watch?v=SjRss8Uk6mQ\n",
    "- https://github.com/derekgreene/topic-model-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab time\n",
    "\n",
    "Pick a text dataset to spend the rest of class working. There are three other datasets in the NLP_data that you can work with: pitchfork album reviews, fake/real news, deadspin, and political lean. Make sure to unzip political lean or fake news. You can also continue to work with the datasets we've already used (data science, yelp, spam.)\n",
    "\n",
    "<br>\n",
    "\n",
    "For the rest of class apply supervised or unsupervised learning techniques to the dataset of your choice. \n",
    "\n",
    "- Build a model that can differentiate between good/bad review, real/fake news, or liberal/conservative leaning or a model that \n",
    "\n",
    "- Predict how many page views a deadspin can get based on its headlines and tags.\n",
    "\n",
    "- Ignore the labels and attempt cluster the articles.\n",
    "\n",
    "- Have fun with the summarizer!!\n",
    "\n",
    "<br>\n",
    "\n",
    "Be prepared to share your results at the end of class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
